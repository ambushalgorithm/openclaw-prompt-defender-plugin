# ğŸ›¡ï¸ openclaw-prompt-defender

<p align="center">
  <img src="https://img.shields.io/badge/Plugin-TypeScript-blue?style=for-the-badge&logo=typescript" alt="TypeScript">
  <img src="https://img.shields.io/badge/Service-Python-cyan?style=for-the-badge&logo=python" alt="Python">
  <img src="https://img.shields.io/badge/OpenClaw-v2026.2.4-green?style=for-the-badge" alt="OpenClaw">
  <img src="https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge" alt="License">
</p>

> Prompt injection detection and jailbreak prevention for OpenClaw â€” scans tool outputs before they reach the LLM.

## âœ¨ What is this?

**openclaw-prompt-defender** is a security plugin that protects your AI assistant from malicious inputs hidden in tool outputs. It intercepts results from tools like `web_fetch`, `exec`, and `read` before they reach the LLM, scanning for:

- ğŸ¯ **Prompt injection attacks** â€” Attempts to override your AI's instructions
- ğŸ”“ **Jailbreak attempts** â€” Tricks to bypass safety guidelines  
- ğŸ”‘ **Secret leaks** â€” Accidental exposure of API keys, tokens, passwords
- ğŸ‘¤ **PII exposure** â€” Personal information that shouldn't be shared
- ğŸ’‰ **Malicious content** â€” XSS, SQL injection, RCE attempts

## ğŸš€ Quick Start

### Prerequisites

- OpenClaw v2026.2.4+
- Python 3.12+
- Docker (for testing)

### Installation

```bash
# Clone the plugin
git clone https://github.com/ambushalgorithm/openclaw-prompt-defender.git
cd openclaw-prompt-defender

# Install Python dependencies
cd service && pip install -r requirements.txt

# Build the plugin
cd ../plugin && npm install
```

### Run the Service

```bash
cd service
python -m app
# Service runs on http://localhost:8080
```

### Configuration

Add to your OpenClaw config:

```json
{
  "plugins": {
    "enabled": ["prompt-defender"],
    "prompt-defender": {
      "url": "http://localhost:8080"
    }
  }
}
```

## ğŸ—ï¸ Architecture

```
User Input â†’ OpenClaw â†’ Tool Execution â†’ [Plugin] â†’ [Scanner Service] â†’ LLM
                                              â†“
                                      Block if malicious
```

**Two-part design:**

1. **Plugin** (TypeScript) â€” Runs in OpenClaw's sandbox, handles hooks
2. **Service** (Python/FastAPI) â€” Runs on host, does pattern matching & ML detection

## ğŸ” Detection Methods

| Method | Patterns | Use Case |
|--------|----------|----------|
| **prompt_guard** | 500+ regex | Core injection detection |
| **ml_detection** | HuggingFace DeBERTa | Advanced ML-based detection |
| **secret_scanner** | 50+ patterns | API keys, tokens, passwords |
| **content_moderation** | OpenAI API | Policy violations |

Each is independently toggleable via feature flags.

## ğŸ“– Usage Examples

### Basic Scanning

```bash
# Scan text for threats
curl -X POST "http://localhost:8080/scan" \
  -H "Content-Type: application/json" \
  -d '{"content": "Hello world"}'
```

### Response

```json
{
  "threat_detected": false,
  "verdict": "ALLOW",
  "matches": []
}
```

### Blocked Content

```json
{
  "threat_detected": true,
  "verdict": "BLOCK",
  "matches": [
    {
      "pattern": "[INST]",
      "type": "prompt_injection",
      "severity": "critical"
    }
  ]
}
```

## ğŸ§ª Testing

We use [prompt-injection-testing](https://github.com/ambushalgorithm/prompt-injection-testing) for generating test samples:

```bash
# Clone test samples service
git clone https://github.com/ambushalgorithm/prompt-injection-testing.git
cd prompt-injection-testing
pip install -r requirements.txt
python main.py  # Runs on port 8081
```

### Run Tests

```bash
# Start scanner service
cd service && python -m app &

# Run tests
pytest -v
```

See [Testing](#testing) section for more details.

## ğŸ“ Project Structure

```
openclaw-prompt-defender/
â”œâ”€â”€ plugin/                 # TypeScript plugin (OpenClaw sandbox)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ index.ts       # before_tool_result hook
â”‚   â””â”€â”€ openclaw.plugin.json
â”‚
â”œâ”€â”€ service/                # Python scanner service
â”‚   â”œâ”€â”€ app.py             # FastAPI /scan endpoint
â”‚   â”œâ”€â”€ scanner.py         # Tiered scanning engine
â”‚   â”œâ”€â”€ patterns.py        # Detection patterns
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ DESIGN.md          # Architecture details
â”‚
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

```json
{
  "features": {
    "prompt_guard": {
      "enabled": true,
      "tier": 2
    },
    "ml_detection": {
      "enabled": false
    }
  },
  "logging": {
    "enabled": true,
    "path": "./logs"
  }
}
```

## ğŸ³ Docker

```bash
# Build and run
cd service
docker build -t prompt-defender .
docker run -d -p 8080:8080 prompt-defender
```

Or use docker-compose:

```bash
docker-compose up -d
```

## ğŸ¤ Contributing

Contributions welcome! Check [TODO.md](TODO.md) for current tasks.

## ğŸ“œ License

MIT License

## ğŸ”— Related Projects

- [prompt-injection-testing](https://github.com/ambushalgorithm/prompt-injection-testing) â€” Test sample generation
- [prompt-guard](https://github.com/seojoonkim/prompt-guard) â€” Regex patterns
- [detect-injection](https://github.com/protectai/detect-injection) â€” ML detection
- [openclaw-shield](https://github.com/knostic/openclaw-shield) â€” Secrets/PII

---

<p align="center">
  <sub>Built with ğŸ”’ for secure AI assistants</sub>
</p>
